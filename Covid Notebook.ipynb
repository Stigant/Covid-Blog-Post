{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adjustText in c:\\users\\liam\\anaconda3\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\liam\\anaconda3\\lib\\site-packages (from adjustText) (3.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\liam\\anaconda3\\lib\\site-packages (from adjustText) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\liam\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\liam\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\liam\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (8.0.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\liam\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\liam\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\liam\\anaconda3\\lib\\site-packages (from matplotlib->adjustText) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\liam\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->adjustText) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('owid-covid-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_fields=['new_cases', 'new_deaths', 'icu_patients']\n",
    "\n",
    "def pull(country='', continent='', start=0, cols=default_fields):\n",
    "    #Pull cols+date data for given country/continent after start days\n",
    "    \n",
    "    file='owid-covid-data.csv'\n",
    "    df=pd.read_csv(file)\n",
    "    if 'date' not in cols:\n",
    "        cols=['date']+cols\n",
    "    if country:\n",
    "        df=df[df.location == country].loc[:,cols]\n",
    "    elif continent: \n",
    "        df=df[df.continent == continent].loc[:,cols]\n",
    "    else:\n",
    "        df=df.loc[:,cols]\n",
    "    df.date=df.date.apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))\n",
    "    s=df.date.iloc[0]\n",
    "    df=df[(df.date-s).dt.days >= start].set_index('date')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{Q1: Which countries have suffered the worst Covid outbreaks?}$$\n",
    "In particular we'd like to find out which countries have the highest death/case counts proprtionate to their population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pull(cols=['location','iso_code','total_deaths_per_million', 'total_cases_per_million']).loc['2021-02-1']\n",
    "df=df.dropna(subset=['iso_code']).drop('iso_code', axis=1).set_index('location').drop('World').dropna()# drop continents/the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_rank=df[['total_deaths_per_million']].sort_values('total_deaths_per_million', ascending=False).take(range(10)).reset_index()\n",
    "deaths_rank.index+=1\n",
    "deaths_rank.columns= ['Country', 'Deaths per million']\n",
    "deaths_rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ten worst outbreaks, as measured by deaths, consist almost entirely of European Countries. The exception being of course the United States coming in at number 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_rank=df[['total_cases_per_million']].sort_values('total_cases_per_million', ascending=False).take(range(10)).reset_index()\n",
    "cases_rank.index+=1\n",
    "cases_rank.columns= ['Country', 'Cases per million']\n",
    "cases_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The countries with the most cases per million are a little less dominated by European Countries. Somewhat surprisngly only 4 countries appear on both lists. This could be a consequence of understated case counts in countries overwhelmed by the disease, or there might be an underlying epidemeiological cause such variations in population demographics making some countries more vulnarable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{Q2: Is there a correlation between population density and outbreak severity?}$$\n",
    "Once again we'll consider both total cases and total deaths as possible measures of outbreak severity. For this question we'll focus mainly on Europe, since this block is relatively similar and heavily affected. Plus there is good quality data on population density.\n",
    "\n",
    "There are also two main measures of population density we might use. The first is the usual. $$Density=\\frac{Population}{Area}$$\n",
    "\n",
    "Alternatively we can use population weighted density. In principle for a country with population $X$ we have\n",
    "\n",
    "$$\\text{PWD}(d,X) = \\frac{1}{|X|}\\sum_{p \\in X}N(p,d)$$\n",
    "where $N(p,d)$ is the number of people living within $d$ of $p$. In practice this number is not computable, but we can break a country up into a set of pixels P_{i} of fixed area $a$, density $d_{i}$ and population $p_{i}$. Then \n",
    "\n",
    "$$\\text{APWD}(a, X) =\\frac{\\sum_{P_{i}} d_{i}p_{i}}{\\sum p_{i}} $$\n",
    "gives an approximate measure, though it depends on our choice of pixels. We will take the population weighted density to be given by $AWD(1,X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='WorldBank Population density.csv'\n",
    "dense= pd.read_csv(file).set_index('Country Name')\n",
    "\n",
    "df=pull(continent='Europe', cols=['location','iso_code','total_deaths_per_million', 'total_cases_per_million']).loc['2021-02-1']\n",
    "df=df.dropna(subset=['iso_code']).drop('iso_code', axis=1).set_index('location').dropna()# drop continents/the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.merge(dense['2018'], how= 'inner', left_on='location', right_index=True)\n",
    "df.rename(columns={'2018':'Population Density'}, inplace=True)\n",
    "def inp(row,n):\n",
    "    if n > 10:\n",
    "        return row\n",
    "    if row['Population Density']!= row['Population Density']:\n",
    "        row['Population Density']= dense.loc[row.name, str(2017-n)]\n",
    "        return inp(row,n+1)\n",
    "    else:\n",
    "        return row\n",
    "df=df.apply(lambda row: inp(row, 0), axis=1).sort_values('Population Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='pwd_country.csv'\n",
    "pwd= pd.read_csv(file)\n",
    "pwd=pwd[['COUNTRY_NAME', '2010']] ## Data for 2020 onwards are just estimates\n",
    "pwd.columns=['Country Name', 'Population Weighted Density']\n",
    "df=df.merge(pwd, left_index=True, right_on='Country Name', how='inner').set_index('Country Name')\n",
    "df.rename(columns={'total_deaths_per_million':'Total Deaths Per Million', 'total_cases_per_million':'Total Cases Per Million'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=sns.lmplot(data=df.drop(['Malta']), y='Total Cases Per Million', x='Population Density', fit_reg=True, aspect=3, height=5)\n",
    "fig.set(xlim=(0, df.drop(['Malta'])['Population Density'].max()+10))\n",
    "\n",
    "labels=[]\n",
    "\n",
    "for country in df.drop(['Malta']).index:\n",
    "     labels.append(plt.text(df.loc[country,'Population Density']+0.01, df.loc[country, 'Total Cases Per Million'], \n",
    "     country))\n",
    "labels\n",
    "adjust_text(labels, arrowprops=dict(arrowstyle=\"->\", color='black'));\n",
    "plt.title('Density vs Cases in Europe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot total cases per million vs population density then there does seem to be something of a correlation. The line of best clearly doesn't give a good description of the relationship, so the Pearson Coefficient will be low. However we would expect a Spearman Rank calculation to find a modest correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=sns.lmplot(data=df, y='Total Deaths Per Million', x='Population Weighted Density', fit_reg=True, aspect=3, height=5)\n",
    "fig.set(xlim=(df['Population Weighted Density'].min()-50, df['Population Weighted Density'].max()+50))\n",
    "\n",
    "labels=[]\n",
    "\n",
    "for country in df.index:\n",
    "     labels.append(plt.text(df.loc[country,'Population Weighted Density']+0.01, df.loc[country, 'Total Deaths Per Million'], \n",
    "     country))\n",
    "labels\n",
    "adjust_text(labels, arrowprops=dict(arrowstyle=\"->\", color='black'));\n",
    "plt.title('Population Weighted Density vs Cases in Europe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very surprisingly, to me at least. There appears to be little relationship at all - the line of best fit is basically flat, and it is a poor desciptor besides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=df.corr(method='pearson').drop(['Population Density', 'Population Weighted Density'], axis=1).merge(\n",
    "    df.corr(method='spearman').drop(['Population Density', 'Population Weighted Density'], axis=1), left_index=True, right_index=True, suffixes=('-Pearson', '-Spearman')).loc[['Population Density', 'Population Weighted Density']]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation calculations backup the graphical observations. Neither measure of density has a strong linear correlation with deaths or cases. On the other hand the Spearman's rank calculations tell us there is some degree of correlation, with the strongest being between Total Cases and Population Density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{Q3: Is there good linear model which predicts deaths based upon case and/or ICU patient numbers?}\n",
    "\n",
    "Many commentators have given a variety of rules of thumb for estimate death rates 1/2 weeks in advance. Intuitively we would expect some essentially fixed proportion of ICU patints/ Covid cases not to survive, so some kind of linear model is a good candidate. Indeed all the estimates I have seen given are of this form.\n",
    "\n",
    "There are of course complicating factors, most noticeably noise introduced to cases and deaths by periodic reporting cycles and that the new cases reported are not always a good estimate of the actual number of cases in the population. \n",
    "\n",
    "To answer this question we'll construct a few models on data from 2020 which we'll test on data from January 2021 to give us an idea of real world performance. Obviously it's not ideal that the test data isn't chosen at random, but it will do for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pull(country='United Kingdom')\n",
    "df.index.rename('Date', inplace=True)\n",
    "df.columns=['New Cases', 'New Deaths', 'ICU Patients']\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(16, 6))\n",
    "ax=sns.lineplot(data=df, x=df.index, y='ICU Patients', color='red', alpha=0.5)\n",
    "sns.lineplot(data=df, x=df.index, y='New Deaths', ax=ax, color='green', alpha=0.5)\n",
    "ax.set(ylabel='')\n",
    "plt.legend(['ICU Patients', 'New Deaths']);\n",
    "ax2=ax.twinx()\n",
    "ax=sns.lineplot(data=df, x=df.index, y='New Cases', ax=ax2)\n",
    "plt.legend(['New Cases']);\n",
    "plt.title('Covid in the UK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some missing data at the beginning of the pandemic. We can fill the NaN's with 0 but for the most part we'll avoid using data before mid-April, even when we have it the quality is very questionable. It can't be seen on the graph but the 2 most recent days of Icu patient numbers are missing too, these rows will have to be dropped.\n",
    "\n",
    "Looking at the data, it does loosely seem, at least in the second half of the year, that the number of deaths follows ICU patients which in turn follows New Cases. As above however, there is a large amount of noise in both the death and case numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(0).iloc[:-2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trail(df,cols,trail_from, trail_to=0, keepNan=False):\n",
    "    #df must be date indexed\n",
    "    #Trail_from and trail_to are either an integers or lists of length =col, can mix and match \n",
    "    #Includes data from between trail_from and trail_to days before\n",
    "    #keepNan=True includes dates for which the trailing window has missing entries\n",
    "    \n",
    "    if not isinstance(trail_from, list) and not isinstance(trail_to, list):  \n",
    "        odf=pd.DataFrame(df[cols].copy())\n",
    "        if not keepNan:\n",
    "            ndf=df.iloc[trail_from:,:]\n",
    "        else:\n",
    "            ndf=df\n",
    "        for i in range(0,trail_from+1,1):\n",
    "            if i >= trail_to:\n",
    "                ndf=ndf.merge(odf, how='left', left_index=True, right_index=True, suffixes=('','_-'+str(i)) )\n",
    "            diff=dt.timedelta(days=1)\n",
    "            odf.index=odf.index+diff\n",
    "        \n",
    "        ndf=ndf.drop(cols,axis=1)  \n",
    "        ndf=ndf\n",
    "        return ndf\n",
    "    else:\n",
    "        if not isinstance(trail_from, list):\n",
    "            trail_from =len(cols)*[trail_from]\n",
    "             \n",
    "        if not isinstance(trail_to, list):\n",
    "            trail_to =len(cols)*[trail_to]\n",
    "        \n",
    "        if len(cols)!=len(trail_from):\n",
    "            raise ValueError('Trail_from must be the same length as cols')\n",
    "        elif len(cols)!=len(trail_to):\n",
    "            raise ValueError('Trail_to must be the same length as cols')\n",
    "        else:\n",
    "            if not keepNan:\n",
    "                ndf=df.iloc[max(trail_from):,:]\n",
    "            else:\n",
    "                ndf=df\n",
    "            \n",
    "            for i in range(len(cols)):\n",
    "                col=cols[i]\n",
    "                n=trail_from[i]\n",
    "                m=trail_to[i]\n",
    "                #print(n,m,col)\n",
    "                ndf=ndf.drop(col,axis=1).merge(trail(pd.DataFrame(df[col]), [col], n,m, keepNan=keepNan)\n",
    "                                          ,how='left', left_index=True, right_index=True, suffixes=('x','y'))\n",
    "            \n",
    "        return ndf\n",
    "def pipe(df, target='New Deaths', predictors=['New Cases', 'ICU Patients'],trailing=['New Cases', 'ICU Patients'], trail_n=21,trail_to=7, smooth=7, cutoffs=(dt.date(2020,1,1)\n",
    ", dt.date(2021, 1, 1))):\n",
    "    #target (str)- variable to model\n",
    "    #trailing (str/list of strs)- categories to include historial data for\n",
    "    #trail_n (int/list of ints) # of days to include takes either single value or one value per category\n",
    "    #trail_to (int/list of ints) # prevents inclusion of data more recent than trail_to days\n",
    "    #smooth (int) - if non-zero replace target by n day trailing average\n",
    "    #cutoffs (pair of datetimes)-restrict attention to dates between those given\n",
    "    \n",
    "    df=df[[target]+predictors]        \n",
    "        \n",
    "    if smooth:\n",
    "        target_series=trail(pd.DataFrame(df[target]), target,smooth).mean(axis=1).rename(target+'_smoothed'+str(smooth))\n",
    "        #print(df.columns)\n",
    "        df=df.drop(target, axis=1)\n",
    "        target=target+'_smoothed'+str(smooth)\n",
    "        #target_Series=target_series.rename(target)\n",
    "    else:\n",
    "        target_series=df[target]\n",
    "        df=df.drop(target, axis=1)\n",
    "        \n",
    "    trail_df=trail(df, trailing ,trail_n, trail_to)\n",
    "    \n",
    "    if cutoffs:\n",
    "        target_series=target_series.loc[cutoffs[0]:cutoffs[1]]\n",
    "    \n",
    "    if trail_df.shape[0] != target_series.shape[0]:\n",
    "        merge_dat=trail_df.merge(target_series, how ='inner', left_index=True, right_index=True, suffixes=('',''))\n",
    "        y=merge_dat.loc[:,target]\n",
    "        X=merge_dat.drop(target, axis=1)\n",
    "    else:\n",
    "        y=target_series\n",
    "        X=trail_df.drop(target)\n",
    "    \n",
    "    \n",
    "    \n",
    "    lm_model=LinearRegression()\n",
    "    lm_model.fit(X,y)\n",
    "    y_pred=lm_model.predict(X)\n",
    "    \n",
    "    return trail_df, X,y, y_pred, lm_model\n",
    "\n",
    "def plotpred(x,y, y_pred):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    ax=sns.lineplot(x=x, y=y_pred)\n",
    "    sns.lineplot(x=x, y=y, ax=ax, color='red', alpha=0.5)\n",
    "    plt.legend(['Prediction','Actual'])\n",
    "    \n",
    "def coefdf(model, cols):\n",
    "\n",
    "    coefs=pd.DataFrame(model.coef_,cols).reset_index()\n",
    "    coefs.columns=['Variable', 'Coefficient']\n",
    "    def rowlabel(row):\n",
    "        label=row[0].split('_')\n",
    "        try: \n",
    "            c=int(label[-1])\n",
    "            if len(label)==3:\n",
    "                x=label[0]+'_'+label[1]\n",
    "            else:\n",
    "                x=label[0]\n",
    "        except:\n",
    "            c=0\n",
    "            if len(label)==2:\n",
    "                x=label[0]+'_'+label[1]\n",
    "            else:\n",
    "                x=label[0]\n",
    "        return x,c, row[1]\n",
    "    coefs=coefs.apply(rowlabel, axis=1, result_type='expand')\n",
    "    coefs.columns=['DataType', 'Days', 'Coefficient']\n",
    "    return coefs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trail_df,X,y, y_pred, lm_model=pipe(df, predictors=['New Cases'], trailing=['New Cases'], trail_n=28, trail_to=6, smooth=0,cutoffs=(dt.date(2020,6,1)\n",
    ", dt.date(2021, 1, 1)))\n",
    "plotpred(y.index, y, y_pred)\n",
    "print('MSE-TRAIN: '+str(mean_squared_error(y_pred,y)))\n",
    "plt.title('Model Estimate - Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a pretty good model by doing linear regression on case data from 1-4 weeks prior to the prediction date. The relatively large number of input variables mean we're in danger of overfitting, however, and unsuprsingly the model performs poorly on the test data as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,X,y, _, _=pipe(df, cutoffs=(dt.date(2021,1, 1)\n",
    ", dt.date(2021, 2, 1)),predictors=['New Cases'], trailing=['New Cases'], trail_to=6,trail_n=28, smooth=0)\n",
    "y_pred=lm_model.predict(X)\n",
    "plotpred(y.index, y, y_pred)\n",
    "print('MSE-TEST: '+str(mean_squared_error(y_pred,y)))\n",
    "plt.title('Model Estimate - Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could of course try including fewer days of data to combat this, but the performance remains poor. Regardless of the number of days included all the models systematically underestimate the number of deaths in the last few months of the year, it is possible this is caused by a fundamental shift in the relationship between case numbes and deaths, due to say the new variant or increasing pressure on the NHS reaching a tipping point.\n",
    "\n",
    "The solution would seem to be to include data on ICU patients as well. Since this data is not subject to the same reporting patterns as deaths and cases it is important to smooth out the target data. We will do this by replacing New Deaths with its 7 day trailing average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=21\n",
    "m=6\n",
    "s=6\n",
    "\n",
    "trail_df,X,y, y_pred, lm_model=pipe(df, predictors=['ICU Patients'], trailing=['ICU Patients'], trail_n=n, trail_to=m, smooth=s, cutoffs=(dt.date(2020,4,15)\n",
    ", dt.date(2021, 1, 1)))\n",
    "plotpred(y.index, y, y_pred)\n",
    "plt.title('Model Estimate - Train')\n",
    "print('MSE-TRAIN: '+str(mean_squared_error(y_pred,y)))\n",
    "_,X,y, _, _=pipe(df, cutoffs=(dt.date(2021,1, 1)\n",
    ", dt.date(2021, 2, 1)),predictors=['ICU Patients'], trailing=['ICU Patients'], trail_n=n,trail_to=m, smooth=s)\n",
    "y_pred=lm_model.predict(X)\n",
    "plotpred(y.index, y, y_pred)\n",
    "plt.title('Model Estimate - Test')\n",
    "print('MSE-TEST: '+str(mean_squared_error(y_pred,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs fantastically, though as before there is some underestimation on the training data in the last couple of months of 2020.\n",
    "\n",
    "We see however that this model largely predicts from the most recent sets of ICU figures. This is the pattern no matter the date range - the best estimate of deaths on a given day is essentially given by whatever the most recent ICU data is. In particular, as the table beloew shows, the model performs quite poorly if we wish to predict more than about 10 days so in advance.\n",
    "\n",
    "There is also a large negative contribution from ICU patients 2 weeks before the prediction date. I'm not sure what, if any, conclusions can be drawn from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefdf(lm_model,X.columns).drop('DataType',axis=1).plot(x='Days', y='Coefficient')\n",
    "plt.title('Model Coefficients')\n",
    "plt.show()\n",
    "\n",
    "def train_model(days):\n",
    "\n",
    "    trail_df,X,y, y_pred, lm_model=pipe(df, predictors=['ICU Patients'], trailing=['ICU Patients'], trail_n=days+15, trail_to=days, smooth=s, cutoffs=(dt.date(2020,4,10+days)\n",
    "                                        , dt.date(2021, 1, 1)))\n",
    "    tr=mean_squared_error(y_pred,y)\n",
    "\n",
    "    _,X,y, _, _=pipe(df, cutoffs=(dt.date(2021,1, 1)\n",
    "                    , dt.date(2021, 2, 1)),predictors=['ICU Patients'], trailing=['ICU Patients'], trail_n=days+15, trail_to=days, smooth=s)\n",
    "    y_pred=lm_model.predict(X)\n",
    "    te=mean_squared_error(y_pred,y)\n",
    "    \n",
    "    return pd.Series([tr,te])\n",
    "\n",
    "res=pd.Series(range(6,15))\n",
    "res.index=range(6,15)\n",
    "res=res.apply(train_model)\n",
    "res.columns=['MSE-Train', 'MSE-Test']\n",
    "res.index.rename('Days in Advance', inplace=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a model that uses both these features to give pretty good performance. By using both case numbers and ICU patient numbers we can predict smoothed deaths reasonably well using only data from 2-3 weeks prior to the prediction date. It is possible that the falling real death count at the end of January, relative to the predicted rate, is a consequence of the vaccines being distributed but it's too early to tell at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=6\n",
    "m=[13,13]\n",
    "n=[17, 17]\n",
    "\n",
    "trail_df,X,y, y_pred, lm_model=pipe(df, cutoffs=(dt.date(2020,4,15)\n",
    ", dt.date(2021, 1, 1)), trail_to=m,trail_n=n, smooth=s)\n",
    "print('MSE-TRAIN: '+str(mean_squared_error(y_pred,y)))\n",
    "plotpred(y.index, y, y_pred)\n",
    "plt.title('Model Estimate - Train')\n",
    "_,X,y, _, _=pipe(df, cutoffs=(dt.date(2021,1, 1)\n",
    ", dt.date(2021, 2, 1)), trail_to=m,trail_n=n, smooth=s)\n",
    "y_pred=lm_model.predict(X)\n",
    "plotpred(y.index, y, y_pred)\n",
    "plt.title('Model Estimate - Test')\n",
    "print('MSE-TEST: '+str(mean_squared_error(y_pred,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're looking for something simpler in your life and want to predict the deaths a couple of weeks in advance, the following model does a suprisngly good job just using cases and icu_patients 2 weeks prior. Curiously this model outperforms the larger one on the test data, this is concievably a consequence of overfitting in the earlier model but is most likely chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=[13,13]\n",
    "n=[13,13]\n",
    "s=6\n",
    "\n",
    "trail_df,X,y, y_pred, lm_model=pipe(df, cutoffs=(dt.date(2020,5,1)\n",
    ", dt.date(2021, 1, 1)), trail_to=m,trail_n=n, smooth=0) ## Not modelled against smoothed data because not enough information provided \n",
    "_,X,y, _, _=pipe(df, cutoffs=(dt.date(2020,5, 1)\n",
    ", dt.date(2021, 1, 1)), trail_to=m,trail_n=n, smooth=s)\n",
    "print('MSE-TRAIN: '+str(mean_squared_error(y_pred,y)))\n",
    "plotpred(y.index, y, y_pred)\n",
    "plt.title('Model Estimate - Train')\n",
    "_,X,y, _, _=pipe(df, cutoffs=(dt.date(2021,1, 1)\n",
    ", dt.date(2021, 2, 1)), trail_to=m,trail_n=n, smooth=s)\n",
    "y_pred=lm_model.predict(X)\n",
    "plotpred(y.index, y, y_pred)\n",
    "plt.title('Model Estimate - Test')\n",
    "print('MSE-TEST: '+str(mean_squared_error(y_pred,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefdf(lm_model,X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall we see that cases are consistently poor indicator of future deaths, \n",
    "\n",
    "Overall, neither ICU patients nor cases are particularly good predictors of reported deaths at 2 weeks; though, after smoothing the data, the number of ICU patients can be used to predict deaths in the next week extremely well. Combined they offer a pretty good estimation at 2 weeks, even if you restrict to just a single days worth of data.\n",
    "\n",
    "We could likely estimate deaths even better if we included a contribution of some kind from the positivity rate to estimate how many cases go unnoticed, it's not clear however exactly how this would be best implented, especially if we wished to ensure that relationship would be well approximated by some linear function. You're welcome to give it a try yourself though, if you have a couple of hours free.\n",
    "\n",
    "There's also this period at the end of 2020 where almost all the models underestimate the number of deaths. Why do you think this is?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
